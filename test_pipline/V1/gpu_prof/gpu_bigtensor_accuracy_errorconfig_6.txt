paddle.mode(Tensor([2, 10, 10],"float64"), 1, )
paddle.moveaxis(Tensor([2, 3, 4, 5, 7],"float64"), list[0,4,3,2,], list[1,3,2,0,], )
paddle.moveaxis(x=Tensor([4, 2, 3, 5, 7],"float64"), source=0, destination=2, )
paddle.moveaxis(x=Tensor([4, 2, 3, 5, 7],"float64"), source=tuple(0,1,), destination=tuple(2,3,), )
paddle.multigammaln(Tensor([10, 20],"float32"), 2, )
paddle.multigammaln(Tensor([10, 20],"float64"), 2, )
paddle.multiplex(inputs=list[Tensor([7, 4],"float32"),Tensor([7, 4],"float32"),], index=Tensor([6, 1],"int32"), )
paddle.multiplex(inputs=list[Tensor([4, 4],"float32"),Tensor([4, 4],"float32"),], index=Tensor([4, 1],"int32"), )
paddle.multiplex(inputs=list[Tensor([4, 4],"float32"),Tensor([4, 4],"float32"),], index=Tensor([2, 1],"int32"), )
paddle.multiply(x=Tensor([128, 256, 56, 56],"float32"), y=Tensor([128, 256, 1, 1],"float32"), )
paddle.multiply(x=Tensor([128, 224, 56, 56],"float32"), y=Tensor([128, 224, 1, 1],"float32"), )
paddle.multiply(Tensor([512, 872, 14, 14],"float32"), Tensor([512, 872, 1, 1],"float32"), )
paddle.mv(Tensor([64, 32],"float64"), Tensor([32],"float64"), )
paddle.mv(Tensor([5, 100],"float64"), Tensor([100],"float64"), )
paddle.mv(Tensor([3, 36],"float32"), Tensor([36],"float32"), )
paddle.nan_to_num(Tensor([1948, 2],"float32"), neginf=-1.1920928955078125e-07, )
paddle.nan_to_num(Tensor([148, 5, 3],"float32"), neginf=-1.1920928955078125e-07, )
paddle.nan_to_num(Tensor([400, 1],"float64"), neginf=-2.220446049250313e-16, )
paddle.nanmean(Tensor([2, 3, 4, 5],"float32"), -1, False, )
paddle.nanmean(Tensor([2, 3, 4, 5],"float32"), 2, True, )
paddle.nanmean(Tensor([2, 3, 4, 5],"float32"), None, False, )
paddle.nanmedian(Tensor([2, 100],"float32"), axis=1, mode="min", )
paddle.nanmedian(Tensor([120],"float32"), keepdim=False, )
paddle.nanmedian(Tensor([120],"float32"), keepdim=False, mode="min", )
paddle.nanquantile(Tensor([4, 7, 6],"float64"), q=0, axis=1, )
paddle.nanquantile(Tensor([4, 7, 6],"float64"), q=0.35, )
paddle.nanquantile(Tensor([4, 7, 6],"float64"), q=0.35, axis=2, keepdim=True, )
paddle.nansum(x=Tensor([3, 2, 3, 4, 5, 1, 2],"float64"), axis=3, keepdim=True, )
paddle.nansum(Tensor([2, 3, 4, 5],"float32"), axis=None, keepdim=False, name=None, )
paddle.nansum(Tensor([2, 3, 4, 5],"float32"), axis=None, keepdim=True, name=None, )
paddle.neg(Tensor([8, 16, 32],"float32"), )
paddle.neg(Tensor([32, 8],"float32"), )
paddle.neg(Tensor([32, 8],"float64"), )
paddle.negative(Tensor([2, 3, 4, 5],"float16"), )
paddle.negative(Tensor([2, 3, 4, 5],"float32"), )
paddle.negative(Tensor([2, 3, 4, 5],"float64"), )
paddle.nextafter(Tensor([2, 3, 4, 5],"float32"), Tensor([2, 3, 4, 5],"float32"), )
paddle.nextafter(Tensor([4, 3, 2],"float32"), Tensor([4, 3, 2],"float64"), )
paddle.nextafter(Tensor([4, 3, 2],"float64"), Tensor([4, 3, 2],"float32"), )
paddle.nn.functional.adaptive_avg_pool1d(Tensor([128, 1536, 49],"float32"), 1, None, )
paddle.nn.functional.adaptive_avg_pool1d(Tensor([64, 1024, 144],"float32"), 1, None, )
paddle.nn.functional.adaptive_avg_pool1d(Tensor([124, 1536, 49],"float32"), 1, None, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2048, 2048, 7, 7],"float32"), output_size=1, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2045, 2048, 7, 7],"float32"), output_size=1, )
paddle.nn.functional.adaptive_avg_pool2d(Tensor([2037, 2048, 7, 7],"float32"), output_size=1, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 7, 10],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 7, 9],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
paddle.nn.functional.adaptive_avg_pool3d(Tensor([1, 768, 16, 9, 7],"float32"), output_size=tuple(1,1,1,), data_format="NCDHW", name=None, )
paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([128, 16],"float32"), Tensor([128],"int64"), Tensor([16, 8],"float32"), list[list[Tensor([16, 8],"float32"),Tensor([8, 5],"float32"),],list[Tensor([16, 4],"float32"),Tensor([4, 5],"float32"),],list[Tensor([16, 2],"float32"),Tensor([2, 5],"float32"),],], list[5,10,15,20,], None, )
paddle.nn.functional.adaptive_log_softmax_with_loss(Tensor([8, 8],"float32"), Tensor([8],"int64"), Tensor([8, 3],"float32"), list[list[Tensor([8, 4],"float32"),Tensor([4, 2],"float32"),],], list[2,4,], None, )
paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 32],"float32"), 16, False, None, )
paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 32],"float32"), output_size=16, )
paddle.nn.functional.adaptive_max_pool1d(Tensor([2, 3, 32],"float64"), 8, False, None, )
paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 7],"float32"), output_size=5, return_mask=False, name=None, )
paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 7],"float32"), output_size=list[2,5,], return_mask=False, name=None, )
paddle.nn.functional.adaptive_max_pool2d(Tensor([2, 3, 7, 7],"float32"), output_size=list[3,3,], return_mask=False, name=None, )
paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 7],"float32"), output_size=5, return_mask=False, name=None, )
paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 7],"float32"), output_size=list[2,3,5,], return_mask=False, name=None, )
paddle.nn.functional.adaptive_max_pool3d(Tensor([2, 3, 5, 7, 7],"float32"), output_size=list[3,3,3,], return_mask=False, name=None, )
paddle.nn.functional.affine_grid(Tensor([20, 2, 3],"float32"), Tensor([4],"int64"), align_corners=True, )
paddle.nn.functional.affine_grid(Tensor([20, 2, 3],"float32"), list[20,1,7,7,], align_corners=True, )
paddle.nn.functional.affine_grid(Tensor([20, 2, 3],"float32"), list[20,2,5,7,], align_corners=False, )
paddle.nn.functional.avg_pool2d(Tensor([16, 128, 256, 256],"float32"), kernel_size=tuple(2,2,), stride=None, padding=0, ceil_mode=False, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.avg_pool2d(Tensor([8, 256, 240, 240],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.avg_pool2d(Tensor([128, 256, 56, 56],"float32"), kernel_size=2, stride=2, padding=0, ceil_mode=True, exclusive=True, divisor_override=None, data_format="NCHW", name=None, )
paddle.nn.functional.avg_pool3d(x=Tensor([8, 2048, 4, 7, 7],"float32"), kernel_size=list[4,7,7,], stride=1, data_format="NCDHW", )
paddle.nn.functional.avg_pool3d(x=Tensor([8, 256, 32, 7, 7],"float32"), kernel_size=list[32,7,7,], stride=1, data_format="NCDHW", )
paddle.nn.functional.avg_pool3d(Tensor([2, 3, 32, 32, 32],"float32"), kernel_size=2, stride=2, padding="SAME", )
paddle.nn.functional.batch_norm(Tensor([30, 40, 50, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), data_format="NHWC", )
paddle.nn.functional.batch_norm(Tensor([30, 40, 50, 60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), Tensor([60],"float32"), use_global_stats=True, data_format="NHWC", )
paddle.nn.functional.batch_norm(Tensor([30, 40, 50, 60],"float32"), Tensor([40],"float32"), Tensor([40],"float32"), Tensor([40],"float32"), Tensor([40],"float32"), )
paddle.nn.functional.bilinear(Tensor([5, 5],"float32"), Tensor([5, 4],"float32"), Tensor([1000, 5, 4],"float32"), Tensor([1, 1000],"float32"), None, )
paddle.nn.functional.bilinear(Tensor([1, 3],"float32"), Tensor([1, 3],"float32"), Tensor([6, 3, 3],"float32"), Tensor([1, 6],"float32"), None, )
paddle.nn.functional.bilinear(Tensor([3, 1],"float32"), Tensor([3, 2],"float32"), Tensor([4, 1, 2],"float32"), Tensor([1, 4],"float32"), None, )
paddle.nn.functional.binary_cross_entropy(Tensor([16, 12096, 80],"float32"), Tensor([16, 12096, 80],"float32"), weight=Tensor([16, 12096, 80],"float32"), reduction="sum", )
paddle.nn.functional.binary_cross_entropy(Tensor([16, 11109, 80],"float32"), Tensor([16, 11109, 80],"float32"), weight=Tensor([16, 11109, 80],"float32"), reduction="sum", )
paddle.nn.functional.binary_cross_entropy(Tensor([16, 10164, 80],"float32"), Tensor([16, 10164, 80],"float32"), weight=Tensor([16, 10164, 80],"float32"), reduction="sum", )
paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([16, 300, 80],"float32"), Tensor([16, 300, 80],"float32"), weight=Tensor([16, 300, 80],"float32"), reduction="none", )
paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([300, 1000],"float32"), Tensor([300, 1000],"float32"), weight=Tensor([300, 1000],"float32"), reduction="none", pos_weight=None, )
paddle.nn.functional.binary_cross_entropy_with_logits(Tensor([512, 28, 28],"float32"), Tensor([512, 28, 28],"float32"), weight=Tensor([512, 1, 1],"float32"), reduction="mean", )
paddle.nn.functional.celu(Tensor([2, 4, 4],"float64"), 0.2, None, )
paddle.nn.functional.celu(Tensor([2, 4, 4],"float64"), 1.0, None, )
paddle.nn.functional.celu(x=Tensor([2, 4, 4],"float64"), )
paddle.nn.functional.channel_shuffle(Tensor([2, 4, 4, 9],"float64"), 3, "NHWC", )
paddle.nn.functional.channel_shuffle(Tensor([2, 4, 4, 9],"float64"), 3, "NHWC", None, )
paddle.nn.functional.channel_shuffle(Tensor([2, 9, 4, 4],"float64"), 3, "NCHW", )
paddle.nn.functional.conv1d(Tensor([32, 32, 58081],"float32"), Tensor([32, 32, 1],"float32"), bias=Tensor([32],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d(Tensor([32, 32, 58081],"float32"), Tensor([1, 32, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d(Tensor([16, 64, 25500],"float32"), Tensor([1, 64, 1],"float32"), bias=Tensor([1],"float32"), padding=0, stride=list[1,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d_transpose(Tensor([2, 512, 7],"float32"), Tensor([512, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d_transpose(Tensor([1, 512, 7],"float32"), Tensor([512, 256, 8],"float32"), bias=Tensor([256],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv1d_transpose(Tensor([2, 256, 28],"float32"), Tensor([256, 128, 8],"float32"), bias=Tensor([128],"float32"), output_size=None, output_padding=0, padding=2, stride=list[4,], dilation=list[1,], groups=1, data_format="NCL", )
paddle.nn.functional.conv2d(Tensor([1024, 1, 260, 260],"float32"), Tensor([1, 1, 4, 4],"float32"), )
paddle.nn.functional.conv2d(Tensor([1024, 1, 259, 259],"float32"), Tensor([1, 1, 4, 4],"float32"), )
paddle.nn.functional.conv2d(Tensor([1024, 1, 258, 258],"float32"), Tensor([1, 1, 4, 4],"float32"), )
paddle.nn.functional.conv2d_transpose(Tensor([8, 64, 480, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([16, 64, 320, 320],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv2d_transpose(Tensor([4, 64, 480, 480],"float32"), Tensor([64, 1, 2, 2],"float32"), bias=Tensor([1],"float32"), padding=0, output_padding=0, stride=list[2,2,], dilation=list[1,1,], groups=1, output_size=None, data_format="NCHW", )
paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 8],"float32"), Tensor([8, 3, 3, 3, 3],"float32"), Tensor([8],"float32"), padding="same", stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.nn.functional.conv3d(Tensor([4, 6, 8, 8, 8],"float32"), Tensor([12, 1, 3, 3, 3],"float32"), None, padding="valid", stride=1, dilation=1, groups=6, data_format="NCDHW", )
paddle.nn.functional.conv3d(Tensor([4, 3, 8, 8, 8],"float32"), Tensor([5, 3, 3, 3, 3],"float32"), Tensor([5],"float32"), padding=list[list[0,0,],list[0,0,],list[1,1,],list[2,2,],list[2,2,],], stride=1, dilation=1, groups=1, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([2, 3, 2, 2, 2],"float32"), Tensor([3, 12, 12, 12, 12],"float32"), bias=Tensor([12],"float32"), padding=0, output_padding=0, stride=list[1,1,1,], dilation=list[1,1,1,], groups=1, output_size=None, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 8],"float32"), Tensor([4, 4, 3, 3, 3],"float32"), Tensor([4],"float32"), output_size=tuple(10,17,10,), padding="valid", stride=tuple(1,2,1,), dilation=1, groups=1, data_format="NCDHW", )
paddle.nn.functional.conv3d_transpose(Tensor([4, 4, 8, 8, 8],"float32"), Tensor([4, 3, 3, 3, 3],"float32"), Tensor([6],"float32"), output_size=None, padding=list[1,1,2,2,1,1,], stride=1, dilation=1, groups=2, data_format="NCDHW", )
paddle.nn.functional.cosine_embedding_loss(Tensor([10, 3],"float32"), Tensor([10, 3],"float32"), Tensor([10],"int64"), margin=0.5, reduction="mean", name=None, )
paddle.nn.functional.cosine_embedding_loss(Tensor([5, 3],"float64"), Tensor([5, 3],"float64"), Tensor([5],"int32"), margin=0.5, reduction="mean", )
paddle.nn.functional.cosine_embedding_loss(Tensor([5, 3],"float64"), Tensor([5, 3],"float64"), Tensor([5],"int32"), margin=0.5, reduction="none", )
paddle.nn.functional.cosine_similarity(Tensor([210, 1024],"float32"), Tensor([210, 1024],"float32"), axis=-1, eps=1e-08, )
paddle.nn.functional.cosine_similarity(Tensor([32, 128],"float32"), Tensor([32, 128],"float32"), )
paddle.nn.functional.cosine_similarity(Tensor([10, 12, 10],"float32"), Tensor([10, 1, 10],"float32"), axis=2, eps=1e-06, )
paddle.nn.functional.cross_entropy(Tensor([8, 1024, 50304],"float32"), Tensor([8, 1024, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
paddle.nn.functional.cross_entropy(Tensor([1, 4096, 100352],"float32"), Tensor([1, 4096, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
paddle.nn.functional.cross_entropy(Tensor([1, 2048, 151936],"float32"), Tensor([1, 2048, 1],"int64"), weight=None, ignore_index=-100, reduction="none", soft_label=False, axis=-1, use_softmax=True, label_smoothing=0.0, name=None, )
paddle.nn.functional.elu(Tensor([1, 21504, 2],"float32"), )
paddle.nn.functional.elu(Tensor([15, 20],"float32"), 1.0, )
paddle.nn.functional.elu(Tensor([10, 20, 1],"float32"), )
paddle.nn.functional.embedding(Tensor([1, 4097],"int64"), weight=Tensor([100352, 8192],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, )
paddle.nn.functional.embedding(Tensor([1, 1024],"int64"), weight=Tensor([151936, 4096],"bfloat16"), padding_idx=None, max_norm=None, norm_type=2.0, sparse=False, scale_grad_by_freq=False, name=None, )
paddle.nn.functional.embedding(Tensor([8, 1024],"int64"), weight=Tensor([50304, 4096],"float16"), padding_idx=None, sparse=False, name=None, )
paddle.nn.functional.fold(Tensor([3, 12, 12],"float32"), output_sizes=list[4,5,], kernel_sizes=2, )
paddle.nn.functional.fold(Tensor([3, 12, 12],"float64"), output_sizes=list[4,5,], kernel_sizes=2, )
paddle.nn.functional.fold(Tensor([3, 12, 12],"float64"), output_sizes=list[4,5,], kernel_sizes=list[2,2,], strides=list[1,1,], paddings=list[0,0,0,0,], dilations=list[1,1,], name=None, )
paddle.nn.functional.gelu(Tensor([128, 96, 96, 768],"float32"), False, None, )
paddle.nn.functional.gelu(Tensor([128, 96, 96, 512],"float32"), False, None, )
paddle.nn.functional.gelu(Tensor([124, 96, 96, 512],"float32"), False, None, )
paddle.nn.functional.glu(Tensor([30, 498, 512],"float32"), -1, None, )
paddle.nn.functional.glu(Tensor([30, 477, 512],"float32"), -1, None, )
paddle.nn.functional.glu(Tensor([30, 457, 512],"float32"), -1, None, )
paddle.nn.functional.grid_sample(x=Tensor([4, 64, 80, 94, 311],"float32"), grid=Tensor([4, 280, 376, 25, 3],"float32"), mode="bilinear", padding_mode="zeros", align_corners=False, )
paddle.nn.functional.grid_sample(Tensor([200, 1, 544, 544],"float32"), Tensor([200, 1, 12544, 2],"float32"), align_corners=False, )
paddle.nn.functional.grid_sample(Tensor([100, 1, 768, 768],"float32"), Tensor([100, 1, 12544, 2],"float32"), align_corners=False, )
paddle.nn.functional.group_norm(Tensor([30, 256, 16, 168],"float32"), num_groups=32, epsilon=1e-05, weight=Tensor([256],"float32"), bias=Tensor([256],"float32"), )
paddle.nn.functional.group_norm(Tensor([30, 256, 24, 112],"float32"), num_groups=32, epsilon=1e-05, weight=Tensor([256],"float32"), bias=Tensor([256],"float32"), )
paddle.nn.functional.group_norm(Tensor([30, 64, 32, 336],"float32"), num_groups=32, epsilon=1e-05, weight=Tensor([64],"float32"), bias=Tensor([64],"float32"), )
paddle.nn.functional.hardshrink(Tensor([3],"float32"), -1, None, )
paddle.nn.functional.hardshrink(Tensor([3],"float32"), 0.5, None, )
paddle.nn.functional.hardshrink(Tensor([3],"float64"), -1, None, )
paddle.nn.functional.hardsigmoid(Tensor([300, 4096],"float32"), )
paddle.nn.functional.hardsigmoid(Tensor([640, 960, 1, 1],"float32"), slope=0.2, offset=0.5, )
paddle.nn.functional.hardsigmoid(Tensor([1024, 576, 1, 1],"float32"), slope=0.2, offset=0.5, )
paddle.nn.functional.hardswish(Tensor([512, 64, 112, 112],"float32"), None, )
paddle.nn.functional.hardswish(Tensor([512, 48, 112, 112],"float32"), None, )
paddle.nn.functional.hardswish(Tensor([256, 80, 112, 112],"float32"), None, )
paddle.nn.functional.hardtanh(Tensor([10, 20, 1],"float32"), -1.0, 1.0, )
paddle.nn.functional.hardtanh(Tensor([3, 3, 3],"float64"), -3.2, -3.2, None, )
paddle.nn.functional.hardtanh(Tensor([3, 3, 3],"float64"), -3.4, 0, None, )
paddle.nn.functional.hinge_embedding_loss(Tensor([10, 10, 5],"float64"), Tensor([10, 10, 5],"float64"), )
paddle.nn.functional.hinge_embedding_loss(Tensor([10, 10, 5],"float64"), Tensor([10, 10, 5],"float64"), reduction="mean", margin=1.0, name=None, )
paddle.nn.functional.hinge_embedding_loss(Tensor([10, 10, 5],"float64"), Tensor([10, 10, 5],"float64"), reduction="none", )
paddle.nn.functional.instance_norm(Tensor([8, 32, 32, 64],"float32"), None, None, Tensor([32],"float32"), Tensor([32],"float32"), True, 0.9, 1e-05, )
paddle.nn.functional.instance_norm(Tensor([8, 32, 32, 64],"float64"), None, None, Tensor([32],"float64"), Tensor([32],"float64"), True, 0.9, 1e-05, )
paddle.nn.functional.instance_norm(Tensor([2, 32, 128],"float32"), )
paddle.nn.functional.interpolate(Tensor([4, 256, 336, 336],"float16"), scale_factor=0.5, align_corners=False, align_mode=0, mode="bilinear", )
paddle.nn.functional.interpolate(Tensor([4, 256, 336, 336],"float32"), scale_factor=0.5, align_corners=False, align_mode=0, mode="bilinear", )
paddle.nn.functional.interpolate(Tensor([4, 256, 336, 320],"float16"), scale_factor=0.5, align_corners=False, align_mode=0, mode="bilinear", )
paddle.nn.functional.l1_loss(Tensor([3548, 12, 170, 1],"float32"), Tensor([3548, 12, 170, 1],"float32"), "none", )
paddle.nn.functional.l1_loss(Tensor([64, 3, 128, 128],"float32"), Tensor([64, 3, 128, 128],"float32"), "mean", name=None, )
paddle.nn.functional.l1_loss(Tensor([16, 511, 257],"float32"), Tensor([16, 511, 257],"float32"), )
paddle.nn.functional.label_smooth(label=Tensor([128, 40, 33712],"float32"), epsilon=0.1, )
paddle.nn.functional.label_smooth(label=Tensor([160, 32, 33712],"float32"), epsilon=0.1, )
paddle.nn.functional.label_smooth(label=Tensor([256, 20, 33712],"float32"), epsilon=0.1, )
paddle.nn.functional.layer_norm(Tensor([7, 435, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
paddle.nn.functional.layer_norm(Tensor([7, 286, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
paddle.nn.functional.layer_norm(Tensor([7, 220, 1024],"float32"), 1024, weight=Tensor([1024],"float32"), bias=Tensor([1024],"float32"), epsilon=1e-05, )
paddle.nn.functional.leaky_relu(Tensor([64, 64, 256, 256],"float32"), 0.1, None, )
paddle.nn.functional.leaky_relu(Tensor([12, 32, 608, 1088],"float32"), 0.1, )
paddle.nn.functional.leaky_relu(Tensor([12, 64, 304, 544],"float32"), 0.1, )
paddle.nn.functional.linear(x=Tensor([2, 25088],"float32"), weight=Tensor([25088, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, )
paddle.nn.functional.linear(x=Tensor([1, 25088],"float32"), weight=Tensor([25088, 4096],"float32"), bias=Tensor([4096],"float32"), name=None, )
paddle.nn.functional.linear(x=Tensor([4096, 12544],"float32"), weight=Tensor([12544, 1024],"float32"), bias=Tensor([1024],"float32"), name=None, )
paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 3, 40, 40],"float32"), size=5, data_format="NCDHW", )
paddle.nn.functional.local_response_norm(x=Tensor([3, 3, 40, 40, 3],"float32"), size=5, data_format="NDHWC", )
paddle.nn.functional.local_response_norm(Tensor([3, 3, 40, 40],"float32"), 5, 0.0001, 0.75, 1.0, "NCHW", None, )
paddle.nn.functional.log_loss(Tensor([102400, 1],"float32"), Tensor([102400, 1],"float32"), epsilon=1e-07, )
paddle.nn.functional.log_loss(Tensor([25600, 1],"float32"), Tensor([25600, 1],"float32"), epsilon=1e-07, )
paddle.nn.functional.log_loss(Tensor([400, 1],"float32"), Tensor([400, 1],"float32"), )
paddle.nn.functional.log_sigmoid(Tensor([10, 10, 10],"float32"), None, )
paddle.nn.functional.log_sigmoid(Tensor([10, 10, 10],"float64"), None, )
paddle.nn.functional.log_sigmoid(x=Tensor([10, 10, 10],"float32"), )
paddle.nn.functional.log_softmax(Tensor([4224, 6629],"float32"), axis=1, )
paddle.nn.functional.log_softmax(Tensor([128, 192612],"float32"), axis=-1, )
paddle.nn.functional.log_softmax(Tensor([2944, 6629],"float32"), axis=1, )
paddle.nn.functional.lp_pool1d(Tensor([2, 3, 32],"float64"), 5.0, 5, 3, 0, False, "NCL", None, )
paddle.nn.functional.lp_pool1d(Tensor([2, 3, 32],"float64"), norm_type=5, kernel_size=5, stride=3, padding=list[0,], )
paddle.nn.functional.lp_pool1d(Tensor([2, 3, 32],"float32"), 4.0, 3, 2, 1, False, "NCL", None, )
paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 32],"float32"), 2, kernel_size=2, stride=1, ceil_mode=False, )
paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 32],"float32"), 2, kernel_size=2, stride=None, ceil_mode=False, )
paddle.nn.functional.lp_pool2d(Tensor([2, 3, 32, 32],"float32"), 2, kernel_size=5, stride=3, ceil_mode=True, )
paddle.nn.functional.margin_ranking_loss(Tensor([128],"float32"), Tensor([128],"float32"), Tensor([128],"float32"), 0.5, "mean", None, )
paddle.nn.functional.margin_ranking_loss(Tensor([10, 10],"float64"), Tensor([10, 10],"float64"), Tensor([10, 10],"float64"), 0.0, "mean", )
paddle.nn.functional.margin_ranking_loss(Tensor([10, 10],"float64"), Tensor([10, 10],"float64"), Tensor([10, 10],"float64"), 0.0, "mean", None, )
paddle.nn.functional.max_pool2d(Tensor([1536, 24, 112, 112],"float32"), kernel_size=3, stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([10, 128, 480, 480],"float32"), kernel_size=2, stride=2, padding=0, return_mask=False, ceil_mode=False, data_format="NCHW", name=None, )
paddle.nn.functional.max_pool2d(Tensor([8, 64, 704, 704],"float32"), kernel_size=3, stride=2, padding=1, )
paddle.nn.functional.max_pool3d(Tensor([8, 64, 16, 112, 112],"float32"), kernel_size=tuple(3,3,3,), stride=2, padding=1, return_mask=False, ceil_mode=False, data_format="NCDHW", name=None, )
paddle.nn.functional.max_pool3d(x=Tensor([8, 320, 4, 56, 56],"float32"), kernel_size=list[1,1,1,], stride=list[1,1,1,], padding=list[0,0,0,], data_format="NCDHW", )
paddle.nn.functional.max_pool3d(x=Tensor([8, 32, 32, 56, 56],"float32"), kernel_size=list[1,1,1,], stride=list[1,1,1,], padding=list[0,0,0,], data_format="NCDHW", )
paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float32"), Tensor([1, 3, 8],"int64"), kernel_size=2, stride=2, output_size=list[1,3,16,], )
paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, )
paddle.nn.functional.max_unpool1d(Tensor([1, 3, 8],"float64"), Tensor([1, 3, 8],"int32"), kernel_size=2, stride=2, padding=0, data_format="NCL", output_size=None, name=None, )
paddle.nn.functional.max_unpool2d(Tensor([64, 8, 86, 39],"float32"), Tensor([64, 8, 86, 39],"int32"), 2, 2, output_size=list[64,8,172,79,], )
paddle.nn.functional.max_unpool2d(Tensor([64, 16, 43, 19],"float32"), Tensor([64, 16, 43, 19],"int32"), 2, 2, output_size=list[64,16,86,39,], )
paddle.nn.functional.max_unpool2d(Tensor([64, 32, 21, 9],"float32"), Tensor([64, 32, 21, 9],"int32"), 2, 2, output_size=list[64,32,43,19,], )
paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int32"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[8,10,12,], name=None, )
paddle.nn.functional.max_unpool3d(Tensor([1, 3, 4, 5, 6],"float64"), Tensor([1, 3, 4, 5, 6],"int64"), list[2,2,2,], stride=list[2,2,2,], padding=list[0,0,0,], data_format="NCDHW", output_size=list[7,9,11,], name=None, )
paddle.nn.functional.maxout(Tensor([100, 4, 3, 3],"float32"), 2, 1, None, )
paddle.nn.functional.maxout(Tensor([100, 4, 3, 3],"float64"), 2, 1, None, )
paddle.nn.functional.maxout(x=Tensor([100, 4, 3, 3],"float32"), groups=2, )
paddle.nn.functional.mish(Tensor([12, 256, 40, 40],"float32"), )
paddle.nn.functional.mish(Tensor([12, 128, 40, 40],"float32"), )
paddle.nn.functional.mish(Tensor([12, 512, 20, 20],"float32"), )
paddle.nn.functional.mse_loss(Tensor([3548, 12, 170, 1],"float32"), Tensor([3548, 12, 170, 1],"float32"), "mean", )
paddle.nn.functional.mse_loss(Tensor([64, 4, 3, 64, 128],"float32"), Tensor([64, 4, 3, 64, 128],"float32"), "none", )
paddle.nn.functional.mse_loss(Tensor([64, 3, 3, 64, 128],"float32"), Tensor([64, 3, 3, 64, 128],"float32"), "none", )
paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5],"float64"), Tensor([5, 5],"float64"), reduction="mean", weight=Tensor([5, 5],"float64"), )
paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5],"float64"), Tensor([5, 5],"float64"), weight=Tensor([5, 5],"float64"), reduction="mean", name=None, )
paddle.nn.functional.multi_label_soft_margin_loss(Tensor([5, 5],"float64"), Tensor([5, 5],"float64"), reduction="mean", weight=None, )
